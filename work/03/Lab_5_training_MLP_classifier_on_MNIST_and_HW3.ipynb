{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"font-size: 14pt;\">Prof. Krzysztof Rybinski</div><br/><br/>\n",
    "<div style=\"font-size: 22pt;\"><b>Artificial Intelligence course</b></div><br/><br/>\n",
    "<div style=\"font-size: 18pt;\">LAB 5.3</div><br/>\n",
    "<div style=\"font-size: 18pt;\">- Predicting handwritten digits in MNIST dataset with MLP</div><br/><br/>\n",
    "<div style=\"font-size: 18pt;\">- Homework 3 described at the end of this Jupyter Notebook</div><br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check MNIST data information\n",
    "# https://en.wikipedia.org/wiki/MNIST_database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-21 08:41:47.450466: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-13T15:29:02.781102Z",
     "start_time": "2018-11-13T15:29:02.369623Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset = mnist.load_data()\n",
    "\n",
    "(Xtrain, ytrain), (Xtest, ytest) = dataset\n",
    "\n",
    "n_train = len(Xtrain)\n",
    "n_test = len(Xtest)\n",
    "\n",
    "n_features = 28*28\n",
    "\n",
    "Xtrain = Xtrain.reshape( n_train, n_features )\n",
    "Xtest  = Xtest.reshape( n_test, n_features )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-13T15:29:02.781102Z",
     "start_time": "2018-11-13T15:29:02.369623Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(784,)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtrain[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "784"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "28*28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow( np.reshape( Xtrain[10002], (28,28) ) , cmap=plt.cm.gray)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-13T15:29:02.787949Z",
     "start_time": "2018-11-13T15:29:02.783514Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 784)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtrain.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-13T15:29:17.565311Z",
     "start_time": "2018-11-13T15:29:17.559317Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 784)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtest.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-13T15:29:32.437992Z",
     "start_time": "2018-11-13T15:29:32.433262Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-13T15:45:27.566378Z",
     "start_time": "2018-11-13T15:45:27.562418Z"
    }
   },
   "outputs": [],
   "source": [
    "clf = MLPClassifier(hidden_layer_sizes=(1,), verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check info on MLPClassifer in sklearn\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-13T15:46:02.148932Z",
     "start_time": "2018-11-13T15:45:28.037088Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.38659692\n",
      "Iteration 2, loss = 2.31960728\n",
      "Iteration 3, loss = 2.30574034\n",
      "Iteration 4, loss = 2.30215797\n",
      "Iteration 5, loss = 2.30142002\n",
      "Iteration 6, loss = 2.30125992\n",
      "Iteration 7, loss = 2.30123763\n",
      "Iteration 8, loss = 2.30124920\n",
      "Iteration 9, loss = 2.30124086\n",
      "Iteration 10, loss = 2.30123460\n",
      "Iteration 11, loss = 2.30124406\n",
      "Iteration 12, loss = 2.30124785\n",
      "Iteration 13, loss = 2.30123205\n",
      "Iteration 14, loss = 2.30124338\n",
      "Iteration 15, loss = 2.30124912\n",
      "Iteration 16, loss = 2.30124199\n",
      "Iteration 17, loss = 2.30125117\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(hidden_layer_sizes=(1,), verbose=True)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(Xtrain, ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on MLPClassifier in module sklearn.neural_network._multilayer_perceptron object:\n",
      "\n",
      "class MLPClassifier(sklearn.base.ClassifierMixin, BaseMultilayerPerceptron)\n",
      " |  MLPClassifier(hidden_layer_sizes=(100,), activation='relu', *, solver='adam', alpha=0.0001, batch_size='auto', learning_rate='constant', learning_rate_init=0.001, power_t=0.5, max_iter=200, shuffle=True, random_state=None, tol=0.0001, verbose=False, warm_start=False, momentum=0.9, nesterovs_momentum=True, early_stopping=False, validation_fraction=0.1, beta_1=0.9, beta_2=0.999, epsilon=1e-08, n_iter_no_change=10, max_fun=15000)\n",
      " |  \n",
      " |  Multi-layer Perceptron classifier.\n",
      " |  \n",
      " |  This model optimizes the log-loss function using LBFGS or stochastic\n",
      " |  gradient descent.\n",
      " |  \n",
      " |  .. versionadded:: 0.18\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  hidden_layer_sizes : tuple, length = n_layers - 2, default=(100,)\n",
      " |      The ith element represents the number of neurons in the ith\n",
      " |      hidden layer.\n",
      " |  \n",
      " |  activation : {'identity', 'logistic', 'tanh', 'relu'}, default='relu'\n",
      " |      Activation function for the hidden layer.\n",
      " |  \n",
      " |      - 'identity', no-op activation, useful to implement linear bottleneck,\n",
      " |        returns f(x) = x\n",
      " |  \n",
      " |      - 'logistic', the logistic sigmoid function,\n",
      " |        returns f(x) = 1 / (1 + exp(-x)).\n",
      " |  \n",
      " |      - 'tanh', the hyperbolic tan function,\n",
      " |        returns f(x) = tanh(x).\n",
      " |  \n",
      " |      - 'relu', the rectified linear unit function,\n",
      " |        returns f(x) = max(0, x)\n",
      " |  \n",
      " |  solver : {'lbfgs', 'sgd', 'adam'}, default='adam'\n",
      " |      The solver for weight optimization.\n",
      " |  \n",
      " |      - 'lbfgs' is an optimizer in the family of quasi-Newton methods.\n",
      " |  \n",
      " |      - 'sgd' refers to stochastic gradient descent.\n",
      " |  \n",
      " |      - 'adam' refers to a stochastic gradient-based optimizer proposed\n",
      " |        by Kingma, Diederik, and Jimmy Ba\n",
      " |  \n",
      " |      Note: The default solver 'adam' works pretty well on relatively\n",
      " |      large datasets (with thousands of training samples or more) in terms of\n",
      " |      both training time and validation score.\n",
      " |      For small datasets, however, 'lbfgs' can converge faster and perform\n",
      " |      better.\n",
      " |  \n",
      " |  alpha : float, default=0.0001\n",
      " |      L2 penalty (regularization term) parameter.\n",
      " |  \n",
      " |  batch_size : int, default='auto'\n",
      " |      Size of minibatches for stochastic optimizers.\n",
      " |      If the solver is 'lbfgs', the classifier will not use minibatch.\n",
      " |      When set to \"auto\", `batch_size=min(200, n_samples)`.\n",
      " |  \n",
      " |  learning_rate : {'constant', 'invscaling', 'adaptive'}, default='constant'\n",
      " |      Learning rate schedule for weight updates.\n",
      " |  \n",
      " |      - 'constant' is a constant learning rate given by\n",
      " |        'learning_rate_init'.\n",
      " |  \n",
      " |      - 'invscaling' gradually decreases the learning rate at each\n",
      " |        time step 't' using an inverse scaling exponent of 'power_t'.\n",
      " |        effective_learning_rate = learning_rate_init / pow(t, power_t)\n",
      " |  \n",
      " |      - 'adaptive' keeps the learning rate constant to\n",
      " |        'learning_rate_init' as long as training loss keeps decreasing.\n",
      " |        Each time two consecutive epochs fail to decrease training loss by at\n",
      " |        least tol, or fail to increase validation score by at least tol if\n",
      " |        'early_stopping' is on, the current learning rate is divided by 5.\n",
      " |  \n",
      " |      Only used when ``solver='sgd'``.\n",
      " |  \n",
      " |  learning_rate_init : float, default=0.001\n",
      " |      The initial learning rate used. It controls the step-size\n",
      " |      in updating the weights. Only used when solver='sgd' or 'adam'.\n",
      " |  \n",
      " |  power_t : float, default=0.5\n",
      " |      The exponent for inverse scaling learning rate.\n",
      " |      It is used in updating effective learning rate when the learning_rate\n",
      " |      is set to 'invscaling'. Only used when solver='sgd'.\n",
      " |  \n",
      " |  max_iter : int, default=200\n",
      " |      Maximum number of iterations. The solver iterates until convergence\n",
      " |      (determined by 'tol') or this number of iterations. For stochastic\n",
      " |      solvers ('sgd', 'adam'), note that this determines the number of epochs\n",
      " |      (how many times each data point will be used), not the number of\n",
      " |      gradient steps.\n",
      " |  \n",
      " |  shuffle : bool, default=True\n",
      " |      Whether to shuffle samples in each iteration. Only used when\n",
      " |      solver='sgd' or 'adam'.\n",
      " |  \n",
      " |  random_state : int, RandomState instance, default=None\n",
      " |      Determines random number generation for weights and bias\n",
      " |      initialization, train-test split if early stopping is used, and batch\n",
      " |      sampling when solver='sgd' or 'adam'.\n",
      " |      Pass an int for reproducible results across multiple function calls.\n",
      " |      See :term:`Glossary <random_state>`.\n",
      " |  \n",
      " |  tol : float, default=1e-4\n",
      " |      Tolerance for the optimization. When the loss or score is not improving\n",
      " |      by at least ``tol`` for ``n_iter_no_change`` consecutive iterations,\n",
      " |      unless ``learning_rate`` is set to 'adaptive', convergence is\n",
      " |      considered to be reached and training stops.\n",
      " |  \n",
      " |  verbose : bool, default=False\n",
      " |      Whether to print progress messages to stdout.\n",
      " |  \n",
      " |  warm_start : bool, default=False\n",
      " |      When set to True, reuse the solution of the previous\n",
      " |      call to fit as initialization, otherwise, just erase the\n",
      " |      previous solution. See :term:`the Glossary <warm_start>`.\n",
      " |  \n",
      " |  momentum : float, default=0.9\n",
      " |      Momentum for gradient descent update. Should be between 0 and 1. Only\n",
      " |      used when solver='sgd'.\n",
      " |  \n",
      " |  nesterovs_momentum : bool, default=True\n",
      " |      Whether to use Nesterov's momentum. Only used when solver='sgd' and\n",
      " |      momentum > 0.\n",
      " |  \n",
      " |  early_stopping : bool, default=False\n",
      " |      Whether to use early stopping to terminate training when validation\n",
      " |      score is not improving. If set to true, it will automatically set\n",
      " |      aside 10% of training data as validation and terminate training when\n",
      " |      validation score is not improving by at least tol for\n",
      " |      ``n_iter_no_change`` consecutive epochs. The split is stratified,\n",
      " |      except in a multilabel setting.\n",
      " |      If early stopping is False, then the training stops when the training\n",
      " |      loss does not improve by more than tol for n_iter_no_change consecutive\n",
      " |      passes over the training set.\n",
      " |      Only effective when solver='sgd' or 'adam'.\n",
      " |  \n",
      " |  validation_fraction : float, default=0.1\n",
      " |      The proportion of training data to set aside as validation set for\n",
      " |      early stopping. Must be between 0 and 1.\n",
      " |      Only used if early_stopping is True.\n",
      " |  \n",
      " |  beta_1 : float, default=0.9\n",
      " |      Exponential decay rate for estimates of first moment vector in adam,\n",
      " |      should be in [0, 1). Only used when solver='adam'.\n",
      " |  \n",
      " |  beta_2 : float, default=0.999\n",
      " |      Exponential decay rate for estimates of second moment vector in adam,\n",
      " |      should be in [0, 1). Only used when solver='adam'.\n",
      " |  \n",
      " |  epsilon : float, default=1e-8\n",
      " |      Value for numerical stability in adam. Only used when solver='adam'.\n",
      " |  \n",
      " |  n_iter_no_change : int, default=10\n",
      " |      Maximum number of epochs to not meet ``tol`` improvement.\n",
      " |      Only effective when solver='sgd' or 'adam'.\n",
      " |  \n",
      " |      .. versionadded:: 0.20\n",
      " |  \n",
      " |  max_fun : int, default=15000\n",
      " |      Only used when solver='lbfgs'. Maximum number of loss function calls.\n",
      " |      The solver iterates until convergence (determined by 'tol'), number\n",
      " |      of iterations reaches max_iter, or this number of loss function calls.\n",
      " |      Note that number of loss function calls will be greater than or equal\n",
      " |      to the number of iterations for the `MLPClassifier`.\n",
      " |  \n",
      " |      .. versionadded:: 0.22\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  classes_ : ndarray or list of ndarray of shape (n_classes,)\n",
      " |      Class labels for each output.\n",
      " |  \n",
      " |  loss_ : float\n",
      " |      The current loss computed with the loss function.\n",
      " |  \n",
      " |  best_loss_ : float\n",
      " |      The minimum loss reached by the solver throughout fitting.\n",
      " |  \n",
      " |  loss_curve_ : list of shape (`n_iter_`,)\n",
      " |      The ith element in the list represents the loss at the ith iteration.\n",
      " |  \n",
      " |  t_ : int\n",
      " |      The number of training samples seen by the solver during fitting.\n",
      " |  \n",
      " |  coefs_ : list of shape (n_layers - 1,)\n",
      " |      The ith element in the list represents the weight matrix corresponding\n",
      " |      to layer i.\n",
      " |  \n",
      " |  intercepts_ : list of shape (n_layers - 1,)\n",
      " |      The ith element in the list represents the bias vector corresponding to\n",
      " |      layer i + 1.\n",
      " |  \n",
      " |  n_features_in_ : int\n",
      " |      Number of features seen during :term:`fit`.\n",
      " |  \n",
      " |      .. versionadded:: 0.24\n",
      " |  \n",
      " |  feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
      " |      Names of features seen during :term:`fit`. Defined only when `X`\n",
      " |      has feature names that are all strings.\n",
      " |  \n",
      " |      .. versionadded:: 1.0\n",
      " |  \n",
      " |  n_iter_ : int\n",
      " |      The number of iterations the solver has run.\n",
      " |  \n",
      " |  n_layers_ : int\n",
      " |      Number of layers.\n",
      " |  \n",
      " |  n_outputs_ : int\n",
      " |      Number of outputs.\n",
      " |  \n",
      " |  out_activation_ : str\n",
      " |      Name of the output activation function.\n",
      " |  \n",
      " |  See Also\n",
      " |  --------\n",
      " |  MLPRegressor : Multi-layer Perceptron regressor.\n",
      " |  BernoulliRBM : Bernoulli Restricted Boltzmann Machine (RBM).\n",
      " |  \n",
      " |  Notes\n",
      " |  -----\n",
      " |  MLPClassifier trains iteratively since at each time step\n",
      " |  the partial derivatives of the loss function with respect to the model\n",
      " |  parameters are computed to update the parameters.\n",
      " |  \n",
      " |  It can also have a regularization term added to the loss function\n",
      " |  that shrinks model parameters to prevent overfitting.\n",
      " |  \n",
      " |  This implementation works with data represented as dense numpy arrays or\n",
      " |  sparse scipy arrays of floating point values.\n",
      " |  \n",
      " |  References\n",
      " |  ----------\n",
      " |  Hinton, Geoffrey E.\n",
      " |      \"Connectionist learning procedures.\" Artificial intelligence 40.1\n",
      " |      (1989): 185-234.\n",
      " |  \n",
      " |  Glorot, Xavier, and Yoshua Bengio. \"Understanding the difficulty of\n",
      " |      training deep feedforward neural networks.\" International Conference\n",
      " |      on Artificial Intelligence and Statistics. 2010.\n",
      " |  \n",
      " |  He, Kaiming, et al. \"Delving deep into rectifiers: Surpassing human-level\n",
      " |      performance on imagenet classification.\" arXiv preprint\n",
      " |      arXiv:1502.01852 (2015).\n",
      " |  \n",
      " |  Kingma, Diederik, and Jimmy Ba. \"Adam: A method for stochastic\n",
      " |      optimization.\" arXiv preprint arXiv:1412.6980 (2014).\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> from sklearn.neural_network import MLPClassifier\n",
      " |  >>> from sklearn.datasets import make_classification\n",
      " |  >>> from sklearn.model_selection import train_test_split\n",
      " |  >>> X, y = make_classification(n_samples=100, random_state=1)\n",
      " |  >>> X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y,\n",
      " |  ...                                                     random_state=1)\n",
      " |  >>> clf = MLPClassifier(random_state=1, max_iter=300).fit(X_train, y_train)\n",
      " |  >>> clf.predict_proba(X_test[:1])\n",
      " |  array([[0.038..., 0.961...]])\n",
      " |  >>> clf.predict(X_test[:5, :])\n",
      " |  array([1, 0, 1, 0, 1])\n",
      " |  >>> clf.score(X_test, y_test)\n",
      " |  0.8...\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      MLPClassifier\n",
      " |      sklearn.base.ClassifierMixin\n",
      " |      BaseMultilayerPerceptron\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, hidden_layer_sizes=(100,), activation='relu', *, solver='adam', alpha=0.0001, batch_size='auto', learning_rate='constant', learning_rate_init=0.001, power_t=0.5, max_iter=200, shuffle=True, random_state=None, tol=0.0001, verbose=False, warm_start=False, momentum=0.9, nesterovs_momentum=True, early_stopping=False, validation_fraction=0.1, beta_1=0.9, beta_2=0.999, epsilon=1e-08, n_iter_no_change=10, max_fun=15000)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  partial_fit(self, X, y, classes=None)\n",
      " |      Update the model with a single iteration over the given data.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The input data.\n",
      " |      \n",
      " |      y : array-like of shape (n_samples,)\n",
      " |          The target values.\n",
      " |      \n",
      " |      classes : array of shape (n_classes,), default=None\n",
      " |          Classes across all calls to partial_fit.\n",
      " |          Can be obtained via `np.unique(y_all)`, where y_all is the\n",
      " |          target vector of the entire dataset.\n",
      " |          This argument is required for the first call to partial_fit\n",
      " |          and can be omitted in the subsequent calls.\n",
      " |          Note that y doesn't need to contain all labels in `classes`.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |          Trained MLP model.\n",
      " |  \n",
      " |  predict(self, X)\n",
      " |      Predict using the multi-layer perceptron classifier.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The input data.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      y : ndarray, shape (n_samples,) or (n_samples, n_classes)\n",
      " |          The predicted classes.\n",
      " |  \n",
      " |  predict_log_proba(self, X)\n",
      " |      Return the log of probability estimates.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : ndarray of shape (n_samples, n_features)\n",
      " |          The input data.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      log_y_prob : ndarray of shape (n_samples, n_classes)\n",
      " |          The predicted log-probability of the sample for each class\n",
      " |          in the model, where classes are ordered as they are in\n",
      " |          `self.classes_`. Equivalent to `log(predict_proba(X))`.\n",
      " |  \n",
      " |  predict_proba(self, X)\n",
      " |      Probability estimates.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The input data.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      y_prob : ndarray of shape (n_samples, n_classes)\n",
      " |          The predicted probability of the sample for each class in the\n",
      " |          model, where classes are ordered as they are in `self.classes_`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.ClassifierMixin:\n",
      " |  \n",
      " |  score(self, X, y, sample_weight=None)\n",
      " |      Return the mean accuracy on the given test data and labels.\n",
      " |      \n",
      " |      In multi-label classification, this is the subset accuracy\n",
      " |      which is a harsh metric since you require for each sample that\n",
      " |      each label set be correctly predicted.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |          Test samples.\n",
      " |      \n",
      " |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      " |          True labels for `X`.\n",
      " |      \n",
      " |      sample_weight : array-like of shape (n_samples,), default=None\n",
      " |          Sample weights.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      score : float\n",
      " |          Mean accuracy of ``self.predict(X)`` wrt. `y`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.ClassifierMixin:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from BaseMultilayerPerceptron:\n",
      " |  \n",
      " |  fit(self, X, y)\n",
      " |      Fit the model to data matrix X and target(s) y.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : ndarray or sparse matrix of shape (n_samples, n_features)\n",
      " |          The input data.\n",
      " |      \n",
      " |      y : ndarray of shape (n_samples,) or (n_samples, n_outputs)\n",
      " |          The target values (class labels in classification, real numbers in\n",
      " |          regression).\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |          Returns a trained MLP model.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self, N_CHAR_MAX=700)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : bool, default=True\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : dict\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      " |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      " |      possible to update each component of a nested object.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      **params : dict\n",
      " |          Estimator parameters.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : estimator instance\n",
      " |          Estimator instance.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[ 5.18536970e-115],\n",
      "       [-8.75708216e-104],\n",
      "       [ 6.69337131e-113],\n",
      "       [-6.05990646e-103],\n",
      "       [ 1.33822847e-109],\n",
      "       [-1.78893567e-111],\n",
      "       [ 7.79470471e-115],\n",
      "       [-1.50586626e-113],\n",
      "       [ 5.79259243e-103],\n",
      "       [-5.47952967e-108],\n",
      "       [-1.42209670e-108],\n",
      "       [ 1.78345489e-115],\n",
      "       [ 1.37425579e-109],\n",
      "       [ 1.50153462e-113],\n",
      "       [ 3.28528221e-112],\n",
      "       [-2.55569439e-103],\n",
      "       [ 2.14530094e-114],\n",
      "       [-1.30669597e-109],\n",
      "       [-1.72706162e-106],\n",
      "       [ 7.89829092e-108],\n",
      "       [-1.99460830e-113],\n",
      "       [-5.17149538e-105],\n",
      "       [ 1.71037528e-115],\n",
      "       [-3.22581984e-110],\n",
      "       [-1.39258120e-109],\n",
      "       [-1.59930676e-108],\n",
      "       [ 2.73509387e-108],\n",
      "       [ 1.73459403e-114],\n",
      "       [ 1.34048475e-108],\n",
      "       [-1.73382540e-114],\n",
      "       [ 1.25182027e-108],\n",
      "       [ 1.04654594e-109],\n",
      "       [-1.15406936e-113],\n",
      "       [-5.15050275e-103],\n",
      "       [ 7.44364467e-104],\n",
      "       [-7.91014321e-113],\n",
      "       [-5.45419448e-105],\n",
      "       [-1.36200156e-110],\n",
      "       [-4.68988800e-111],\n",
      "       [ 5.97422026e-110],\n",
      "       [-1.22404815e-105],\n",
      "       [ 6.65405624e-102],\n",
      "       [ 6.93566592e-113],\n",
      "       [-1.27081997e-105],\n",
      "       [ 7.01030777e-107],\n",
      "       [-1.63677346e-112],\n",
      "       [ 2.47073850e-103],\n",
      "       [-1.35934428e-102],\n",
      "       [-2.54013119e-104],\n",
      "       [ 1.92662751e-108],\n",
      "       [-3.67775166e-105],\n",
      "       [-2.22325693e-103],\n",
      "       [ 3.08099883e-113],\n",
      "       [-3.59776791e-111],\n",
      "       [-1.37392964e-113],\n",
      "       [-1.50628611e-110],\n",
      "       [-4.63970229e-110],\n",
      "       [-2.45668561e-102],\n",
      "       [-4.23056972e-114],\n",
      "       [-4.70744671e-109],\n",
      "       [ 1.01803190e-112],\n",
      "       [ 6.03423876e-115],\n",
      "       [-3.37905582e-108],\n",
      "       [-8.07374851e-113],\n",
      "       [-2.64182567e-105],\n",
      "       [ 4.22928757e-103],\n",
      "       [-5.02212987e-115],\n",
      "       [-1.69653049e-116],\n",
      "       [ 3.98730786e-107],\n",
      "       [-1.85335494e-114],\n",
      "       [ 3.17309339e-106],\n",
      "       [ 5.16123809e-002],\n",
      "       [ 3.56490364e-002],\n",
      "       [-5.69463120e-003],\n",
      "       [ 3.33712281e-002],\n",
      "       [ 6.28751298e-002],\n",
      "       [ 4.23495231e-003],\n",
      "       [-4.38241833e-108],\n",
      "       [ 1.29742332e-113],\n",
      "       [-1.65792625e-104],\n",
      "       [-3.50985734e-107],\n",
      "       [-1.17643467e-109],\n",
      "       [ 3.21676280e-103],\n",
      "       [-2.17814552e-106],\n",
      "       [ 1.43478213e-109],\n",
      "       [-3.13606539e-111],\n",
      "       [-8.72549436e-113],\n",
      "       [-6.28668198e-106],\n",
      "       [ 2.52692153e-109],\n",
      "       [ 1.47248637e-108],\n",
      "       [ 6.55703190e-112],\n",
      "       [ 1.75537960e-115],\n",
      "       [ 3.27486841e-113],\n",
      "       [ 6.85002836e-112],\n",
      "       [-1.31226905e-105],\n",
      "       [ 1.32736593e-105],\n",
      "       [ 1.17138466e-002],\n",
      "       [ 1.92769501e-002],\n",
      "       [ 3.93180422e-002],\n",
      "       [-3.64810774e-002],\n",
      "       [ 6.39284725e-002],\n",
      "       [-7.92605983e-002],\n",
      "       [-6.96975404e-002],\n",
      "       [ 7.50307629e-002],\n",
      "       [ 1.53901249e-002],\n",
      "       [-1.10306716e-105],\n",
      "       [ 2.19730522e-112],\n",
      "       [ 1.23585881e-109],\n",
      "       [-3.50390191e-105],\n",
      "       [-1.40333779e-109],\n",
      "       [-2.44397155e-111],\n",
      "       [ 1.30023415e-113],\n",
      "       [ 3.60704039e-115],\n",
      "       [ 2.34927593e-112],\n",
      "       [-5.23010433e-109],\n",
      "       [-1.06334445e-104],\n",
      "       [-3.48663055e-107],\n",
      "       [ 3.93237979e-104],\n",
      "       [ 1.28616104e-106],\n",
      "       [ 7.33053246e-002],\n",
      "       [ 6.34886558e-002],\n",
      "       [ 3.66290234e-002],\n",
      "       [-2.22907286e-002],\n",
      "       [-3.46584367e-002],\n",
      "       [-7.59275856e-002],\n",
      "       [-8.79417928e-002],\n",
      "       [ 5.12692410e-002],\n",
      "       [ 6.32177631e-002],\n",
      "       [ 2.44651327e-002],\n",
      "       [ 4.79522309e-002],\n",
      "       [-3.30904091e-003],\n",
      "       [ 1.64479159e-002],\n",
      "       [ 1.18531477e-002],\n",
      "       [-4.14627588e-002],\n",
      "       [-4.50267985e-002],\n",
      "       [ 7.97744864e-002],\n",
      "       [-7.36271593e-002],\n",
      "       [ 1.88201216e-112],\n",
      "       [-1.51103220e-110],\n",
      "       [-1.47410632e-109],\n",
      "       [-3.21324148e-108],\n",
      "       [-1.43243066e-101],\n",
      "       [ 7.01936418e-116],\n",
      "       [ 1.20197951e-103],\n",
      "       [-8.55931807e-113],\n",
      "       [-1.78651475e-102],\n",
      "       [-6.22852437e-002],\n",
      "       [-5.12793214e-002],\n",
      "       [ 5.29213813e-003],\n",
      "       [ 6.03870400e-002],\n",
      "       [ 6.38612257e-002],\n",
      "       [ 4.18847925e-003],\n",
      "       [-5.56830937e-002],\n",
      "       [ 8.75145534e-003],\n",
      "       [ 3.05056023e-002],\n",
      "       [-3.71493477e-002],\n",
      "       [ 4.75631940e-002],\n",
      "       [-5.57068816e-002],\n",
      "       [-2.94954906e-003],\n",
      "       [ 1.67888690e-002],\n",
      "       [-1.01996442e-002],\n",
      "       [-5.37457859e-002],\n",
      "       [-4.12641525e-002],\n",
      "       [ 4.98343814e-002],\n",
      "       [ 6.81549002e-002],\n",
      "       [ 4.54595111e-002],\n",
      "       [ 6.78354123e-002],\n",
      "       [-1.35020256e-109],\n",
      "       [-2.73368641e-102],\n",
      "       [ 3.03305554e-115],\n",
      "       [ 2.42635192e-112],\n",
      "       [ 5.52498464e-108],\n",
      "       [ 2.47417770e-003],\n",
      "       [ 3.09909453e-002],\n",
      "       [-3.57857824e-002],\n",
      "       [-4.92355315e-002],\n",
      "       [ 2.57078620e-002],\n",
      "       [-2.76812207e-002],\n",
      "       [-5.84037077e-002],\n",
      "       [-1.95919579e-002],\n",
      "       [-9.68211529e-002],\n",
      "       [-8.50149080e-002],\n",
      "       [-7.33892639e-002],\n",
      "       [-2.25671871e-002],\n",
      "       [-7.98848530e-002],\n",
      "       [-9.06252696e-002],\n",
      "       [ 6.14517623e-002],\n",
      "       [-1.59087083e-003],\n",
      "       [ 5.37230757e-002],\n",
      "       [-1.77352206e-002],\n",
      "       [ 2.97158624e-002],\n",
      "       [ 6.99884406e-002],\n",
      "       [-1.48860656e-002],\n",
      "       [-7.89820024e-002],\n",
      "       [-5.44322055e-002],\n",
      "       [-1.11427301e-110],\n",
      "       [-1.29778245e-105],\n",
      "       [ 7.56441911e-115],\n",
      "       [ 6.14252142e-002],\n",
      "       [-6.98722190e-002],\n",
      "       [-6.53729048e-003],\n",
      "       [ 2.41954564e-002],\n",
      "       [ 5.71536118e-002],\n",
      "       [ 1.94181990e-002],\n",
      "       [-7.23929619e-002],\n",
      "       [-9.59397689e-002],\n",
      "       [ 6.17850057e-002],\n",
      "       [-3.34214613e-002],\n",
      "       [-9.21632974e-002],\n",
      "       [ 4.98649434e-002],\n",
      "       [-1.74823867e-002],\n",
      "       [-3.02889798e-002],\n",
      "       [ 4.92996837e-003],\n",
      "       [-8.31998272e-002],\n",
      "       [-3.14804222e-002],\n",
      "       [ 4.70258344e-002],\n",
      "       [-2.95179178e-002],\n",
      "       [ 6.41809246e-002],\n",
      "       [-5.83666618e-002],\n",
      "       [-3.79493774e-002],\n",
      "       [-1.89326713e-002],\n",
      "       [ 9.36639771e-003],\n",
      "       [-1.01545542e-103],\n",
      "       [-1.00148488e-109],\n",
      "       [ 4.21134667e-110],\n",
      "       [ 4.87857778e-002],\n",
      "       [-1.40779334e-003],\n",
      "       [ 3.04920282e-002],\n",
      "       [ 4.26550885e-002],\n",
      "       [ 3.34797767e-002],\n",
      "       [-7.71425915e-003],\n",
      "       [-6.65622472e-003],\n",
      "       [ 5.53015525e-002],\n",
      "       [-1.06778773e-002],\n",
      "       [-8.47883255e-003],\n",
      "       [ 3.29935459e-002],\n",
      "       [-2.38311459e-002],\n",
      "       [ 5.86509968e-002],\n",
      "       [-4.63643500e-002],\n",
      "       [-4.23045771e-002],\n",
      "       [-3.87885009e-002],\n",
      "       [-2.67331874e-004],\n",
      "       [-2.15776984e-002],\n",
      "       [ 1.46558762e-002],\n",
      "       [-1.33437497e-002],\n",
      "       [-3.38836712e-002],\n",
      "       [ 7.00020397e-002],\n",
      "       [ 7.53063998e-002],\n",
      "       [ 3.48750884e-002],\n",
      "       [-9.08517692e-002],\n",
      "       [ 4.53412895e-110],\n",
      "       [ 4.26812311e-110],\n",
      "       [-1.34743906e-106],\n",
      "       [-2.88431065e-002],\n",
      "       [-4.15914358e-002],\n",
      "       [ 5.17301490e-002],\n",
      "       [ 7.37763441e-002],\n",
      "       [-1.70071040e-002],\n",
      "       [-5.58044952e-002],\n",
      "       [-8.38315137e-002],\n",
      "       [ 2.00010605e-003],\n",
      "       [-1.96574287e-002],\n",
      "       [ 6.12445338e-002],\n",
      "       [-7.34860866e-002],\n",
      "       [-7.52650694e-003],\n",
      "       [ 4.11735930e-003],\n",
      "       [ 7.13665637e-002],\n",
      "       [-8.23448155e-003],\n",
      "       [-5.78988645e-002],\n",
      "       [ 5.59686058e-003],\n",
      "       [ 1.25966066e-002],\n",
      "       [-9.09805797e-002],\n",
      "       [-1.25529511e-002],\n",
      "       [-2.24021532e-003],\n",
      "       [-2.50477155e-002],\n",
      "       [-5.78096181e-002],\n",
      "       [ 2.04123089e-002],\n",
      "       [ 2.12135817e-002],\n",
      "       [-5.63594391e-108],\n",
      "       [ 6.20138959e-105],\n",
      "       [-8.12887948e-115],\n",
      "       [-1.57955699e-002],\n",
      "       [-1.13637375e-002],\n",
      "       [-8.02905893e-002],\n",
      "       [-1.37618176e-002],\n",
      "       [-2.90649400e-002],\n",
      "       [-4.75800565e-002],\n",
      "       [-7.35641450e-002],\n",
      "       [-5.64605863e-002],\n",
      "       [-2.74769305e-002],\n",
      "       [ 1.35307354e-002],\n",
      "       [-6.14082197e-002],\n",
      "       [ 4.86952026e-002],\n",
      "       [ 6.45050353e-002],\n",
      "       [-5.44696726e-002],\n",
      "       [ 4.19583903e-002],\n",
      "       [-2.98348559e-002],\n",
      "       [ 1.16977697e-002],\n",
      "       [ 4.80048297e-002],\n",
      "       [-2.93073392e-002],\n",
      "       [-2.11377148e-003],\n",
      "       [-1.09813206e-002],\n",
      "       [ 1.80044503e-002],\n",
      "       [-3.50759175e-002],\n",
      "       [-4.01899376e-002],\n",
      "       [-5.09297871e-002],\n",
      "       [ 3.12675967e-109],\n",
      "       [-5.62271275e-105],\n",
      "       [ 5.03543318e-111],\n",
      "       [ 1.72448334e-106],\n",
      "       [-1.69977120e-102],\n",
      "       [-1.52205178e-107],\n",
      "       [-4.04894517e-002],\n",
      "       [ 5.09591360e-002],\n",
      "       [ 2.91952543e-002],\n",
      "       [-4.87270978e-002],\n",
      "       [-5.75289726e-002],\n",
      "       [-1.70017941e-002],\n",
      "       [ 1.22929825e-002],\n",
      "       [-2.82636319e-002],\n",
      "       [ 5.34806823e-002],\n",
      "       [-6.94502645e-002],\n",
      "       [ 1.48319216e-002],\n",
      "       [-8.52542492e-002],\n",
      "       [ 2.61586251e-002],\n",
      "       [ 6.01691565e-002],\n",
      "       [-7.21286240e-002],\n",
      "       [ 5.79081841e-002],\n",
      "       [-5.09076461e-002],\n",
      "       [-9.48051970e-002],\n",
      "       [-5.66885693e-002],\n",
      "       [ 5.32660382e-002],\n",
      "       [ 3.01331439e-002],\n",
      "       [ 7.49507914e-002],\n",
      "       [ 7.48482453e-002],\n",
      "       [ 1.46472124e-110],\n",
      "       [-5.66869704e-112],\n",
      "       [-9.09560530e-002],\n",
      "       [ 1.79433309e-002],\n",
      "       [-1.73659638e-003],\n",
      "       [ 7.01509926e-002],\n",
      "       [ 7.53313126e-002],\n",
      "       [-3.60150505e-002],\n",
      "       [-2.85453634e-002],\n",
      "       [-2.40368885e-002],\n",
      "       [ 4.43514513e-002],\n",
      "       [-4.59634970e-002],\n",
      "       [-3.77001735e-002],\n",
      "       [-3.56422506e-003],\n",
      "       [-4.90689964e-002],\n",
      "       [-4.46635644e-002],\n",
      "       [-1.45251857e-002],\n",
      "       [-3.37029494e-002],\n",
      "       [ 3.91782330e-002],\n",
      "       [-7.63214973e-002],\n",
      "       [ 7.55293343e-002],\n",
      "       [ 1.01863726e-002],\n",
      "       [-7.54315507e-002],\n",
      "       [ 2.63354664e-002],\n",
      "       [ 1.97518611e-002],\n",
      "       [-5.78289904e-002],\n",
      "       [ 7.62284127e-002],\n",
      "       [ 6.02616596e-002],\n",
      "       [ 2.81754205e-002],\n",
      "       [ 7.76540408e-115],\n",
      "       [-7.30623187e-002],\n",
      "       [ 2.40652124e-003],\n",
      "       [ 1.27095739e-002],\n",
      "       [ 5.03088690e-002],\n",
      "       [ 3.57495730e-002],\n",
      "       [-4.97422728e-002],\n",
      "       [-8.22080783e-002],\n",
      "       [ 4.58847827e-002],\n",
      "       [ 7.04420276e-002],\n",
      "       [-4.33184249e-002],\n",
      "       [-2.43217933e-002],\n",
      "       [ 3.37670649e-002],\n",
      "       [ 6.68973731e-002],\n",
      "       [-5.23737598e-002],\n",
      "       [ 2.08020448e-002],\n",
      "       [-8.97791054e-002],\n",
      "       [-6.16136006e-002],\n",
      "       [-3.77126644e-003],\n",
      "       [-1.93131119e-002],\n",
      "       [-5.15687548e-002],\n",
      "       [-7.83279862e-002],\n",
      "       [-2.41908970e-002],\n",
      "       [ 3.26275346e-002],\n",
      "       [-5.99889558e-002],\n",
      "       [-6.35232521e-002],\n",
      "       [ 4.40921109e-002],\n",
      "       [-1.19219619e-002],\n",
      "       [-1.12135995e-113],\n",
      "       [-1.24152972e-103],\n",
      "       [ 5.01520804e-111],\n",
      "       [-5.39730924e-002],\n",
      "       [-1.75086165e-002],\n",
      "       [-8.87054983e-002],\n",
      "       [ 3.24259643e-002],\n",
      "       [ 3.76479667e-002],\n",
      "       [ 5.43432635e-002],\n",
      "       [-7.83461494e-002],\n",
      "       [-8.65234680e-002],\n",
      "       [-3.74870832e-002],\n",
      "       [ 6.42253805e-002],\n",
      "       [-4.61526682e-002],\n",
      "       [ 6.35413448e-002],\n",
      "       [-5.37162850e-002],\n",
      "       [-5.57539149e-002],\n",
      "       [ 2.29109846e-002],\n",
      "       [-5.86027491e-002],\n",
      "       [-5.44550171e-002],\n",
      "       [-8.77404843e-002],\n",
      "       [-6.97268381e-002],\n",
      "       [ 6.29206947e-002],\n",
      "       [-8.63426738e-003],\n",
      "       [ 1.81943901e-002],\n",
      "       [-1.68754931e-002],\n",
      "       [ 1.90442521e-002],\n",
      "       [-4.03587148e-002],\n",
      "       [-4.18537365e-111],\n",
      "       [-5.57407125e-105],\n",
      "       [ 8.10152906e-111],\n",
      "       [-9.23843729e-113],\n",
      "       [-6.33253095e-002],\n",
      "       [ 7.79074124e-002],\n",
      "       [-6.76227772e-002],\n",
      "       [-1.59504407e-002],\n",
      "       [-4.72867793e-002],\n",
      "       [ 6.98715755e-002],\n",
      "       [ 1.42232181e-003],\n",
      "       [-5.19295654e-002],\n",
      "       [ 4.31302825e-003],\n",
      "       [-4.65859626e-002],\n",
      "       [-9.59205066e-002],\n",
      "       [-9.85556476e-002],\n",
      "       [-8.56257324e-003],\n",
      "       [ 6.54358598e-002],\n",
      "       [-5.58293030e-002],\n",
      "       [ 1.94761543e-002],\n",
      "       [-1.02341316e-001],\n",
      "       [ 1.10697833e-002],\n",
      "       [ 9.90366846e-003],\n",
      "       [-2.93548373e-002],\n",
      "       [ 5.47559340e-002],\n",
      "       [-3.18843823e-002],\n",
      "       [ 6.58732514e-002],\n",
      "       [ 2.01713965e-107],\n",
      "       [-2.51699643e-113],\n",
      "       [ 1.33581890e-110],\n",
      "       [-3.37152444e-102],\n",
      "       [ 8.24204556e-113],\n",
      "       [-7.65742180e-002],\n",
      "       [-5.24464368e-002],\n",
      "       [-8.09043553e-002],\n",
      "       [-7.81886210e-002],\n",
      "       [-8.25833683e-002],\n",
      "       [-6.78409555e-002],\n",
      "       [ 7.31528904e-002],\n",
      "       [-8.82110832e-002],\n",
      "       [-3.24937195e-002],\n",
      "       [-8.34238482e-002],\n",
      "       [-2.29942158e-002],\n",
      "       [-6.91133891e-002],\n",
      "       [-7.71067563e-002],\n",
      "       [-8.34279886e-002],\n",
      "       [ 6.16710202e-002],\n",
      "       [ 5.56061926e-002],\n",
      "       [-5.30164397e-002],\n",
      "       [-8.07016426e-002],\n",
      "       [ 5.21553651e-002],\n",
      "       [-4.85878403e-002],\n",
      "       [-8.60461428e-002],\n",
      "       [ 3.22197894e-002],\n",
      "       [-6.57088818e-002],\n",
      "       [ 3.36460516e-102],\n",
      "       [-1.92553946e-113],\n",
      "       [-3.84312426e-102],\n",
      "       [ 2.75462086e-102],\n",
      "       [ 4.45915782e-112],\n",
      "       [ 5.82706539e-002],\n",
      "       [-3.99833377e-002],\n",
      "       [ 3.61902930e-002],\n",
      "       [-2.44435658e-002],\n",
      "       [ 7.46454890e-002],\n",
      "       [ 6.98506049e-002],\n",
      "       [ 7.25066961e-002],\n",
      "       [-8.36713997e-002],\n",
      "       [ 3.34402469e-002],\n",
      "       [ 3.67193360e-002],\n",
      "       [ 4.52509690e-002],\n",
      "       [ 6.76287170e-002],\n",
      "       [-1.53791783e-002],\n",
      "       [-1.69570215e-002],\n",
      "       [ 3.59729307e-002],\n",
      "       [ 4.13688518e-002],\n",
      "       [ 2.82436608e-002],\n",
      "       [-1.19142314e-002],\n",
      "       [ 5.82943970e-002],\n",
      "       [ 5.46167677e-002],\n",
      "       [ 6.04936504e-002],\n",
      "       [-8.58361847e-002],\n",
      "       [ 3.52345590e-002],\n",
      "       [-2.63683250e-104],\n",
      "       [ 7.15659030e-115],\n",
      "       [-2.06892294e-104],\n",
      "       [ 6.28672775e-112],\n",
      "       [-6.32274607e-103],\n",
      "       [ 7.62089174e-002],\n",
      "       [-6.87319026e-003],\n",
      "       [-4.39227012e-002],\n",
      "       [-6.55700488e-002],\n",
      "       [ 6.54269734e-002],\n",
      "       [ 1.42859230e-004],\n",
      "       [-2.82296126e-002],\n",
      "       [-2.09752310e-002],\n",
      "       [-7.44021365e-002],\n",
      "       [-7.89897306e-002],\n",
      "       [ 5.48565256e-002],\n",
      "       [ 2.93200217e-002],\n",
      "       [-2.51641697e-002],\n",
      "       [-9.38708881e-002],\n",
      "       [-3.26910627e-002],\n",
      "       [-7.36054612e-002],\n",
      "       [ 3.94149507e-002],\n",
      "       [-4.66160805e-002],\n",
      "       [-9.28956191e-002],\n",
      "       [-4.79812888e-002],\n",
      "       [-1.01120915e-002],\n",
      "       [-1.62731854e-111],\n",
      "       [-1.75033580e-112],\n",
      "       [-2.73299760e-107],\n",
      "       [ 1.91060446e-109],\n",
      "       [ 3.55713601e-108],\n",
      "       [-1.84926233e-101],\n",
      "       [ 3.15040260e-113],\n",
      "       [-7.40057494e-002],\n",
      "       [-1.73787090e-002],\n",
      "       [ 3.18255702e-002],\n",
      "       [-9.40115461e-002],\n",
      "       [ 6.13179737e-002],\n",
      "       [-8.56229078e-004],\n",
      "       [-1.33229151e-002],\n",
      "       [-1.45285924e-002],\n",
      "       [-2.09187812e-002],\n",
      "       [-5.19145031e-002],\n",
      "       [-8.40541034e-002],\n",
      "       [-2.10397732e-002],\n",
      "       [ 5.77377203e-003],\n",
      "       [ 7.40034170e-002],\n",
      "       [-8.17194443e-002],\n",
      "       [-7.34912459e-002],\n",
      "       [-4.13655790e-002],\n",
      "       [ 3.51020928e-002],\n",
      "       [-6.19943600e-002],\n",
      "       [-8.05917527e-002],\n",
      "       [ 6.80829370e-116],\n",
      "       [ 1.87088910e-113],\n",
      "       [ 4.59230542e-116],\n",
      "       [ 4.45652009e-112],\n",
      "       [ 4.00832054e-108],\n",
      "       [-4.36335165e-117],\n",
      "       [ 2.37937177e-113],\n",
      "       [ 2.70485279e-102],\n",
      "       [-4.37093304e-002],\n",
      "       [ 5.81780328e-002],\n",
      "       [ 6.01441502e-002],\n",
      "       [ 3.36329526e-002],\n",
      "       [ 7.37896756e-003],\n",
      "       [ 7.22024216e-002],\n",
      "       [ 6.05382952e-002],\n",
      "       [-9.19153483e-002],\n",
      "       [-1.97640625e-002],\n",
      "       [-9.77141977e-003],\n",
      "       [-4.14430399e-002],\n",
      "       [-6.87251211e-002],\n",
      "       [-8.59607677e-002],\n",
      "       [-7.25862608e-002],\n",
      "       [-5.23000664e-002],\n",
      "       [ 4.89155105e-002],\n",
      "       [-7.98283862e-004],\n",
      "       [-2.52658593e-002],\n",
      "       [ 6.23692184e-002],\n",
      "       [-7.53188865e-002],\n",
      "       [ 8.26357957e-104],\n",
      "       [-1.32210473e-113],\n",
      "       [ 1.87098266e-112],\n",
      "       [-1.20828340e-105],\n",
      "       [-1.56043913e-111],\n",
      "       [ 4.45618579e-108],\n",
      "       [ 1.62850903e-113],\n",
      "       [ 1.27028592e-113],\n",
      "       [-1.30761095e-003],\n",
      "       [ 2.88106439e-003],\n",
      "       [ 4.45573785e-002],\n",
      "       [-7.10886009e-002],\n",
      "       [-9.44819501e-002],\n",
      "       [ 3.92981433e-002],\n",
      "       [-1.19793137e-002],\n",
      "       [ 8.15574904e-003],\n",
      "       [-5.29032971e-003],\n",
      "       [-2.14740240e-002],\n",
      "       [-3.23980665e-002],\n",
      "       [-5.84219478e-002],\n",
      "       [ 1.40809811e-003],\n",
      "       [ 7.32004162e-002],\n",
      "       [-7.58088101e-002],\n",
      "       [-9.33817206e-003],\n",
      "       [-6.63825516e-002],\n",
      "       [-5.35129479e-002],\n",
      "       [ 2.70891701e-002],\n",
      "       [ 6.28525557e-002],\n",
      "       [-7.52105794e-110],\n",
      "       [ 2.27810777e-113],\n",
      "       [ 5.27319700e-107],\n",
      "       [ 5.11900200e-105],\n",
      "       [-9.20667787e-114],\n",
      "       [ 4.54695766e-110],\n",
      "       [-1.50193275e-108],\n",
      "       [-1.69246171e-115],\n",
      "       [ 5.25276390e-002],\n",
      "       [-6.24330437e-002],\n",
      "       [ 5.38374431e-002],\n",
      "       [-5.85701701e-002],\n",
      "       [-4.22722939e-002],\n",
      "       [-5.10766307e-002],\n",
      "       [-5.79874144e-002],\n",
      "       [ 5.54685747e-002],\n",
      "       [-7.00716969e-002],\n",
      "       [ 3.58364814e-002],\n",
      "       [-5.78808194e-003],\n",
      "       [-3.71689197e-002],\n",
      "       [-4.24998876e-002],\n",
      "       [-5.64433370e-002],\n",
      "       [-7.28434954e-002],\n",
      "       [ 3.12639077e-002],\n",
      "       [ 7.65414191e-002],\n",
      "       [ 7.29834474e-003],\n",
      "       [-3.84186770e-002],\n",
      "       [ 2.30543061e-002],\n",
      "       [ 1.48120973e-002],\n",
      "       [-2.80382905e-113],\n",
      "       [ 1.30923022e-105],\n",
      "       [-2.38710156e-106],\n",
      "       [-8.05988771e-114],\n",
      "       [ 3.19971509e-105],\n",
      "       [-4.03634299e-106],\n",
      "       [-2.02515408e-107],\n",
      "       [-7.51315004e-002],\n",
      "       [ 6.96455620e-002],\n",
      "       [ 2.40624409e-003],\n",
      "       [ 5.04272135e-003],\n",
      "       [-3.84204678e-002],\n",
      "       [-1.59056153e-002],\n",
      "       [ 5.89580883e-003],\n",
      "       [ 1.28099785e-002],\n",
      "       [ 6.82981438e-002],\n",
      "       [-3.12603933e-002],\n",
      "       [-8.88822150e-002],\n",
      "       [-5.44249110e-002],\n",
      "       [-8.00506535e-002],\n",
      "       [-8.95179858e-004],\n",
      "       [ 5.21091693e-002],\n",
      "       [-4.27115160e-002],\n",
      "       [-8.57285592e-002],\n",
      "       [ 8.35279393e-002],\n",
      "       [ 1.46287064e-002],\n",
      "       [ 2.81001719e-004],\n",
      "       [ 1.48848626e-106],\n",
      "       [-1.24243013e-113],\n",
      "       [-2.14346606e-104],\n",
      "       [-3.12872465e-106],\n",
      "       [-4.11940783e-104],\n",
      "       [ 8.40404796e-112],\n",
      "       [-1.50702087e-110],\n",
      "       [ 5.60310584e-105],\n",
      "       [ 4.22070240e-002],\n",
      "       [ 3.16008004e-002],\n",
      "       [-7.79022322e-002],\n",
      "       [ 4.66411218e-002],\n",
      "       [ 3.28874087e-002],\n",
      "       [ 5.67904017e-002],\n",
      "       [-8.88056011e-003],\n",
      "       [ 1.63270233e-002],\n",
      "       [ 2.63433586e-002],\n",
      "       [-3.55670197e-002],\n",
      "       [ 4.30711043e-003],\n",
      "       [ 7.52653946e-002],\n",
      "       [-4.72841554e-002],\n",
      "       [ 3.40520057e-002],\n",
      "       [ 7.36356087e-002],\n",
      "       [ 4.50888407e-002],\n",
      "       [ 4.76960878e-002],\n",
      "       [-3.08117730e-109],\n",
      "       [ 8.71840934e-109],\n",
      "       [ 7.47362711e-114],\n",
      "       [ 6.25662667e-105],\n",
      "       [ 1.00850592e-107],\n",
      "       [-1.70826995e-111],\n",
      "       [ 1.48344057e-111],\n",
      "       [ 2.85943275e-113],\n",
      "       [-8.02577719e-115],\n",
      "       [ 9.25484006e-110],\n",
      "       [-3.20323504e-109],\n",
      "       [-2.12096512e-111],\n",
      "       [ 9.14404631e-108],\n",
      "       [-8.16416670e-002],\n",
      "       [ 7.25406288e-002],\n",
      "       [-4.88511650e-002],\n",
      "       [-7.02801400e-002],\n",
      "       [ 1.30627826e-002],\n",
      "       [ 7.30165781e-002],\n",
      "       [-3.35415942e-002],\n",
      "       [ 3.57069835e-002],\n",
      "       [ 9.95281569e-003],\n",
      "       [-7.36720319e-002],\n",
      "       [ 5.44473352e-002],\n",
      "       [ 5.27065684e-002],\n",
      "       [ 4.40648364e-002],\n",
      "       [-7.41024282e-002],\n",
      "       [-4.36246310e-002],\n",
      "       [ 2.69496818e-108],\n",
      "       [-9.83669310e-106],\n",
      "       [-1.19984385e-110],\n",
      "       [ 4.21491145e-113],\n",
      "       [ 9.19308661e-110],\n",
      "       [-4.55136874e-109],\n",
      "       [ 5.67953090e-111],\n",
      "       [ 6.74591522e-104],\n",
      "       [ 2.61473062e-115],\n",
      "       [-2.22419711e-112],\n",
      "       [ 1.42617261e-107],\n",
      "       [ 2.70916920e-107],\n",
      "       [ 5.64579074e-105],\n",
      "       [-2.74392157e-002],\n",
      "       [-2.97695405e-002],\n",
      "       [-5.12212460e-002],\n",
      "       [ 4.77164588e-002],\n",
      "       [-8.34758930e-002],\n",
      "       [-6.66474017e-002],\n",
      "       [ 2.86518791e-002],\n",
      "       [ 2.69025118e-002],\n",
      "       [ 4.04880226e-002],\n",
      "       [ 3.10286180e-002],\n",
      "       [-5.69166228e-002],\n",
      "       [-1.19624476e-003],\n",
      "       [ 1.19574599e-002],\n",
      "       [ 8.51432963e-109],\n",
      "       [-2.42596499e-110],\n",
      "       [-3.14647406e-107],\n",
      "       [ 3.38289942e-114],\n",
      "       [-5.61477972e-108],\n",
      "       [ 1.12936168e-103],\n",
      "       [ 1.54668114e-111],\n",
      "       [ 4.64187486e-108],\n",
      "       [-2.13154677e-108],\n",
      "       [ 5.97872086e-112],\n",
      "       [ 1.55864672e-108],\n",
      "       [ 5.12645954e-114],\n",
      "       [ 2.61621495e-113],\n",
      "       [-1.29391873e-115],\n",
      "       [ 4.62491650e-105],\n",
      "       [ 2.32159292e-003],\n",
      "       [ 8.05484848e-002],\n",
      "       [-9.95128667e-003],\n",
      "       [-3.03037057e-002],\n",
      "       [ 2.17252722e-002],\n",
      "       [-4.15015351e-002],\n",
      "       [-4.03856315e-109],\n",
      "       [-2.69522619e-002],\n",
      "       [ 5.23371663e-002],\n",
      "       [-3.01437813e-003],\n",
      "       [-6.71787379e-002],\n",
      "       [-7.62807055e-002],\n",
      "       [-3.23742233e-002],\n",
      "       [-2.46080640e-103],\n",
      "       [-1.58468537e-115],\n",
      "       [ 1.32269604e-105],\n",
      "       [-3.45434173e-108],\n",
      "       [ 4.90920493e-111],\n",
      "       [-2.09978097e-105],\n",
      "       [ 1.09768334e-108],\n",
      "       [ 2.95483280e-109],\n",
      "       [-7.99023480e-106]]), array([[ 0.63566098, -0.63567825, -0.16315971,  0.6495102 ,  0.4489255 ,\n",
      "        -0.55092502,  0.03576571,  0.15541694, -0.45582852,  0.57441388]])]\n"
     ]
    }
   ],
   "source": [
    "print(clf.coefs_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(clf.coefs_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(784, 1)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.coefs_[0].shape\n",
    "#explain the dimension of the W matrix for the first (hidden) layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 10)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.coefs_[1].shape\n",
    "#explain the dimension of the W matrix for the second (output) layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1,)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.intercepts_[0].shape\n",
    "#explain the dimension of the b (bias) vector for the first (hidden) layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10,)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.intercepts_[1].shape\n",
    "#explain the dimension of the b (bias) vector for the second (output) layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-13T15:46:05.771427Z",
     "start_time": "2018-11-13T15:46:05.279063Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.11236666666666667"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(Xtrain, ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-13T15:46:06.285050Z",
     "start_time": "2018-11-13T15:46:06.215576Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1135"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(Xtest, ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow( np.reshape( Xtest[120], (28,28) ) , cmap=plt.cm.gray)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1], dtype=uint8)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.predict(Xtest[120].reshape(1, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-13T15:46:46.642228Z",
     "start_time": "2018-11-13T15:46:46.615128Z"
    }
   },
   "outputs": [],
   "source": [
    "class MyMLP:\n",
    "    def __init__(self, n_input, n_hidden, n_output, random_seed = 0, activate='sigmoid', verbose=False):\n",
    "        np.random.seed( random_seed )\n",
    "        # biases for hidden layer\n",
    "        self.b_hidden = np.random.randn( 1,n_hidden )\n",
    "        # weights for hidden layer\n",
    "        self.w_hidden = np.random.randn( n_input, n_hidden ) / np.sqrt(n_input+n_hidden)    # Glorot Initialization\n",
    "        # biases for output layer\n",
    "        self.b_output = np.random.randn( 1, n_output )\n",
    "        # weights for hidden layer\n",
    "        self.w_output = np.random.randn( n_hidden, n_output ) / np.sqrt(n_hidden+n_output)\n",
    "        \n",
    "        self.verbose = verbose\n",
    "        if activate=='sigmoid':\n",
    "            self.activate = self.sigmoid\n",
    "            self.activate_der = self.sigmoid_der\n",
    "        else:\n",
    "            self.activate = self.tanh\n",
    "            self.activate_der = self.tanh_der\n",
    "\n",
    "        \n",
    "    def sigmoid(self,x):\n",
    "        return 1.0/(1.0+np.exp(-x))\n",
    "\n",
    "    def sigmoid_der(self,x):\n",
    "        g = 1.0/(1.0+np.exp(-x))\n",
    "        return g, g*(1.0-g)\n",
    "    \n",
    "    def tanh(self,x):\n",
    "        return np.tanh(x)\n",
    "    \n",
    "    def tanh_der(self,x):\n",
    "        g = np.tanh(x)\n",
    "        return g, 1.0-g**2\n",
    "\n",
    "    def softmax(self,x):\n",
    "        x -= np.max(x,axis=1,keepdims=True)\n",
    "        x  = np.exp(x)\n",
    "        x /= np.sum(x,axis=1,keepdims=True)\n",
    "        return x\n",
    "        \n",
    "    def predict(self, X):\n",
    "        # X.shape = (m,n_input)\n",
    "        Z1 = X.dot(self.w_hidden) + self.b_hidden\n",
    "        # Z1.shape = (m,n_hidden)\n",
    "        A1 = self.activate( Z1 )\n",
    "        # A1.shape = (m,n_hidden)\n",
    "        Z2 = A1.dot(self.w_output) + self.b_output\n",
    "        # Z2.shape = (m,n_output)\n",
    "        A2 = self.softmax( Z2 )\n",
    "        # A2.shape = (m,n_output)\n",
    "        return A2\n",
    "    \n",
    "    def predict_class(self, X):\n",
    "        yhat = self.predict(X)\n",
    "        # pred.shape = (m,n_output)\n",
    "        # np.argmax( pred , axis=1 ).shape = (m,)\n",
    "        return np.argmax( yhat , axis=1 )\n",
    "    \n",
    "    def score(self,X,y):\n",
    "        return np.mean( self.predict_class(X) == y )\n",
    "    \n",
    "    # cross-entropy\n",
    "    def loss(self, X, y):\n",
    "        yhat = self.predict(X)\n",
    "        return - np.mean( np.log( yhat[ range(len(yhat)), y ] ) )\n",
    "    \n",
    "    def fit(self, Xtrain, ytrain, epochs = 100, learning_rate = 0.1):\n",
    "        m,_ = Xtrain.shape\n",
    "        \n",
    "        for iter in range(epochs):  \n",
    "            # Forward propagation\n",
    "            # X.shape = (m,n_input)\n",
    "            Z1 = Xtrain.dot(self.w_hidden) + self.b_hidden\n",
    "            # Z1.shape = (m,n_hidden)\n",
    "            A1,dZ1 = self.activate_der( Z1 )\n",
    "            # A1.shape = (m,n_hidden)\n",
    "            Z2 = A1.dot(self.w_output) + self.b_output\n",
    "            # Z2.shape = (m,n_output)\n",
    "            A2 = self.softmax( Z2 )\n",
    "            # A2.shape = (m,n_output)\n",
    "\n",
    "            # Backward propagation\n",
    "#             delta2 = (A2-ytrain_one_hot)/m\n",
    "            delta2 = A2\n",
    "            delta2[ range(len(delta2)), ytrain ] -= 1\n",
    "            delta2 /= len(delta2)\n",
    "        \n",
    "            delta1 = delta2.dot( self.w_output.T ) * dZ1\n",
    "\n",
    "            dw_output = A1.T.dot(delta2)\n",
    "            dw_hidden = Xtrain.T.dot(delta1)\n",
    "\n",
    "            db_output = np.sum( delta2, axis=0, keepdims=True )\n",
    "            db_hidden = np.sum( delta1, axis=0, keepdims=True )\n",
    "            \n",
    "            \n",
    "            # Gradient descent\n",
    "            self.w_hidden -= (learning_rate * dw_hidden)\n",
    "            self.b_hidden -= (learning_rate * db_hidden)\n",
    "            self.w_output -= (learning_rate * dw_output)\n",
    "            self.b_output -= (learning_rate * db_output)\n",
    "            \n",
    "            if self.verbose:\n",
    "                print(\"Loss after iteration %i: %f (score=%.2f%%)\" %(iter, self.loss(Xtrain, ytrain), 100.0*self.score(Xtrain,ytrain)))\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-13T15:47:48.517951Z",
     "start_time": "2018-11-13T15:47:48.511334Z"
    }
   },
   "outputs": [],
   "source": [
    "mlp = MyMLP(1, 1, 1, random_seed=0, activate='tanh', verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-13T16:00:47.505723Z",
     "start_time": "2018-11-13T15:57:33.640998Z"
    }
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (60000,784) and (1,1) not aligned: 784 (dim 1) != 1 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [27]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmlp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m \u001b[49m\u001b[43mXtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mytrain\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [25]\u001b[0m, in \u001b[0;36mMyMLP.fit\u001b[0;34m(self, Xtrain, ytrain, epochs, learning_rate)\u001b[0m\n\u001b[1;32m     69\u001b[0m m,_ \u001b[38;5;241m=\u001b[39m Xtrain\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28miter\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):  \n\u001b[1;32m     72\u001b[0m     \u001b[38;5;66;03m# Forward propagation\u001b[39;00m\n\u001b[1;32m     73\u001b[0m     \u001b[38;5;66;03m# X.shape = (m,n_input)\u001b[39;00m\n\u001b[0;32m---> 74\u001b[0m     Z1 \u001b[38;5;241m=\u001b[39m \u001b[43mXtrain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mw_hidden\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mb_hidden\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;66;03m# Z1.shape = (m,n_hidden)\u001b[39;00m\n\u001b[1;32m     76\u001b[0m     A1,dZ1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivate_der( Z1 )\n",
      "\u001b[0;31mValueError\u001b[0m: shapes (60000,784) and (1,1) not aligned: 784 (dim 1) != 1 (dim 0)"
     ]
    }
   ],
   "source": [
    "mlp.fit( Xtrain, ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-13T16:00:47.585501Z",
     "start_time": "2018-11-13T16:00:47.508352Z"
    }
   },
   "outputs": [],
   "source": [
    "mlp.score( Xtest, ytest )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 3\n",
    "Load MNIST fashion dataset, it has the same dimensions as MNIST dataset <br/>\n",
    "https://keras.io/api/datasets/fashion_mnist/  <br/> \n",
    "Go to the sklearn MLPClassifier website, learn about parameters and try different ones to achieve the \n",
    "best accuracy on the test set <br/>\n",
    "Comment on bias (overfitting) and variance (underfitting) <br/>\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from keras.datasets import mnist\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from keras.datasets import fashion_mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = fashion_mnist.load_data()\n",
    "\n",
    "(Xtrain, ytrain), (Xtest, ytest) = dataset\n",
    "\n",
    "n_train = len(Xtrain)\n",
    "n_test = len(Xtest)\n",
    "\n",
    "n_features = 28*28\n",
    "\n",
    "Xtrain = Xtrain.reshape( n_train, n_features )\n",
    "Xtest  = Xtest.reshape( n_test, n_features )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(784,)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtrain[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow( np.reshape( Xtrain[5000], (28,28) ) , cmap=plt.cm.gray)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ytrain[20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MLPClassifier(hidden_layer_sizes=(28*28,100,50,25,10),max_iter= 20,activation = 'relu',\n",
    "                    solver='adam',learning_rate_init=0.01,random_state=1, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "90 overfits with 28*28 392 196 112 56 28 1 of 98% 89% with relu<br>\n",
    "134 overfits with 28*28 98 84 56, 28, 14, 10 of 98% 89% with relu<br>\n",
    "50  overfits with 28*28 96 48 24 10 of 95% 89% with relu <br>\n",
    "50 overfits with 28*28 48 28 10 of 94% 88% with relu<br>\n",
    "50 underfit but acceptable with 28*28 28 28 28 10 of 92% 88% with relu<br>\n",
    "62 underfit with 28*28 48 28 10 of 86% 85% whith identity<br>\n",
    "91 underfit with 28*28 28 10 of 0.86% 0.83% identity<br>\n",
    "50 underfit with 28*28 48 28 10 of 78% 78% whith logistic<br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 3.26929657\n",
      "Iteration 2, loss = 2.30373588\n",
      "Iteration 3, loss = 2.30373077\n",
      "Iteration 4, loss = 2.30379214\n",
      "Iteration 5, loss = 2.30385570\n",
      "Iteration 6, loss = 2.30378207\n",
      "Iteration 7, loss = 2.30374245\n",
      "Iteration 8, loss = 2.30387725\n",
      "Iteration 9, loss = 2.30392879\n",
      "Iteration 10, loss = 2.30387713\n",
      "Iteration 11, loss = 2.30381141\n",
      "Iteration 12, loss = 2.30380804\n",
      "Iteration 13, loss = 2.30377857\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "56.46983289718628"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "clf.fit(Xtrain, ytrain)\n",
    "stoptime = time.time() -start;\n",
    "stoptime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(Xtrain, ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8771"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(Xtest, ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
